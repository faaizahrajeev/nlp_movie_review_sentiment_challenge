{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9dae5be",
   "metadata": {},
   "source": [
    "## creating the dataframe in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353f69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6535e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "data = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e8ceb",
   "metadata": {},
   "source": [
    "## regular expressions to clean the data and normalize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4515100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    text = ' '.join([WordNetLemmatizer().lemmatize(word) for word in text.split()])\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "data['Processed_Review'] = data['Review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192818b2",
   "metadata": {},
   "source": [
    "## ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3ce2058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Necessary for nltk's word_tokenize function, if not already installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8f08417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['can not', 'not believe', 'believe interesting', 'interesting movie', 'movie plot', 'plot great']\n",
      "Trigrams: ['can not believe', 'not believe interesting', 'believe interesting movie', 'interesting movie plot', 'movie plot great']\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n=2):\n",
    "    # First, clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Tokenize the cleaned text\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Generate and return n-grams\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"I can't believe how interesting this movie is, but the plot isn't great!\"\n",
    "bigrams = generate_ngrams(sample_text, 2)  # Generate bigrams\n",
    "trigrams = generate_ngrams(sample_text, 3)  # Generate trigrams\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b33dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data['cleaned_reviews'] = data['Review'].apply(clean_text)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  \n",
    "X = vectorizer.fit_transform(data['cleaned_reviews'])\n",
    "y = data['Sentiment'].map({'Positive': 1, 'Negative': 0})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2fa6c",
   "metadata": {},
   "source": [
    "## naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7918a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data['Processed_Review'])\n",
    "y = data['Sentiment'].map({'Positive': 1, 'Negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704d45a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       124\n",
      "           1       1.00      1.00      1.00       116\n",
      "\n",
      "    accuracy                           1.00       240\n",
      "   macro avg       1.00      1.00      1.00       240\n",
      "weighted avg       1.00      1.00      1.00       240\n",
      "\n",
      "CV Average Accuracy: 0.91 (+/- 0.35)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "cv_scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "print(\"CV Average Accuracy: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843f570",
   "metadata": {},
   "source": [
    "## vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfbf40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "def document_vector(word_vecs, doc):\n",
    "    doc = [word for word in doc if word in word_vecs.key_to_index]\n",
    "    if doc:\n",
    "        return np.mean([word_vecs[word] for word in doc], axis=0)\n",
    "    else:\n",
    "        return np.zeros(word_vecs.vector_size)\n",
    "\n",
    "data['vector'] = data['Processed_Review'].apply(lambda x: document_vector(word_vectors, x.split()))\n",
    "\n",
    "X = np.vstack(data['vector'].values)\n",
    "y = data['Sentiment'].map({'Positive': 1, 'Negative': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ac12d",
   "metadata": {},
   "source": [
    "## neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4906f9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 2s 19ms/step - loss: 0.4304 - accuracy: 0.8615 - val_loss: 0.1326 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0933 - accuracy: 0.9979 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0220 - accuracy: 0.9990 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 6.7267e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.0186e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.6710e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 9.7972e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 5.7861e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.6929e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 9.3728e-04 - accuracy: 1.0000 - val_loss: 2.7137e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.9715e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - ETA: 0s - loss: 6.5568e-04 - accuracy: 1.00 - 0s 6ms/step - loss: 7.2676e-04 - accuracy: 1.0000 - val_loss: 1.4379e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 9.0543e-04 - accuracy: 1.0000 - val_loss: 1.0982e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 6.0117e-04 - accuracy: 1.0000 - val_loss: 7.8955e-06 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 5.2917e-04 - accuracy: 1.0000 - val_loss: 6.1309e-06 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 3.4974e-04 - accuracy: 1.0000 - val_loss: 4.9799e-06 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 4.1107e-04 - accuracy: 1.0000 - val_loss: 4.2770e-06 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 3.4172e-04 - accuracy: 1.0000 - val_loss: 3.5896e-06 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.1103e-04 - accuracy: 1.0000 - val_loss: 3.1017e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 3.5217e-04 - accuracy: 1.0000 - val_loss: 2.4985e-06 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cec0ec59e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=100),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "002db4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4985e-06 - accuracy: 1.0000\n",
      "Test Loss: 2.4985042728076223e-06\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {scores[0]}\")\n",
    "print(f\"Test Accuracy: {scores[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e1c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, word_vectors):\n",
    "    processed_text = clean_text(text)\n",
    "    vector = document_vector(word_vectors, processed_text.split())\n",
    "    vector = vector.reshape(1, -1)\n",
    "    prediction = model.predict(vector)[0][0]\n",
    "    return \"Positive\" if prediction >= 0.5 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb6f8e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "sample_review = \"This movie was a fantastic journey through the realms of science fiction.\"\n",
    "predicted_sentiment = predict_sentiment(sample_review, model, word_vectors)\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
